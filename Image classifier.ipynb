{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 150, 150\n",
    "\n",
    "#defining the data directories\n",
    "train_data_dir= 'data/training_set'\n",
    "validation_data_dir= 'data/test_set'\n",
    "n_training_sample= 4001\n",
    "n_validation_sample= 1011\n",
    "epochs=40\n",
    "batch_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(2, activation=tf.nn.sigmoid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8005 images belonging to 2 classes.\n",
      "Found 2023 images belonging to 2 classes.\n",
      "Epoch 1/40\n",
      "400/400 [==============================] - 102s 256ms/step - loss: 0.7001 - accuracy: 0.4876 - val_loss: 0.6931 - val_accuracy: 0.4941\n",
      "Epoch 2/40\n",
      "400/400 [==============================] - 90s 225ms/step - loss: 0.6931 - accuracy: 0.4958 - val_loss: 0.6931 - val_accuracy: 0.4980\n",
      "Epoch 3/40\n",
      "400/400 [==============================] - 92s 230ms/step - loss: 0.6931 - accuracy: 0.5120 - val_loss: 0.6931 - val_accuracy: 0.4901\n",
      "Epoch 4/40\n",
      "400/400 [==============================] - 90s 225ms/step - loss: 0.6931 - accuracy: 0.5025 - val_loss: 0.6931 - val_accuracy: 0.5089\n",
      "Epoch 5/40\n",
      "400/400 [==============================] - 47s 118ms/step - loss: 0.6931 - accuracy: 0.5134 - val_loss: 0.6931 - val_accuracy: 0.5129\n",
      "Epoch 6/40\n",
      "400/400 [==============================] - 46s 116ms/step - loss: 0.6931 - accuracy: 0.4920 - val_loss: 0.6931 - val_accuracy: 0.5010\n",
      "Epoch 7/40\n",
      "400/400 [==============================] - 50s 124ms/step - loss: 0.6931 - accuracy: 0.4989 - val_loss: 0.6931 - val_accuracy: 0.4871\n",
      "Epoch 8/40\n",
      "400/400 [==============================] - 49s 123ms/step - loss: 0.6931 - accuracy: 0.5029 - val_loss: 0.6931 - val_accuracy: 0.4950\n",
      "Epoch 9/40\n",
      "400/400 [==============================] - 50s 125ms/step - loss: 0.6931 - accuracy: 0.5056 - val_loss: 0.6931 - val_accuracy: 0.4871\n",
      "Epoch 10/40\n",
      "400/400 [==============================] - 46s 114ms/step - loss: 0.6931 - accuracy: 0.4964 - val_loss: 0.6931 - val_accuracy: 0.4891\n",
      "Epoch 11/40\n",
      "400/400 [==============================] - 48s 120ms/step - loss: 0.6931 - accuracy: 0.5034 - val_loss: 0.6931 - val_accuracy: 0.5020\n",
      "Epoch 12/40\n",
      "400/400 [==============================] - 49s 122ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6931 - val_accuracy: 0.4950\n",
      "Epoch 13/40\n",
      "400/400 [==============================] - 49s 123ms/step - loss: 0.6931 - accuracy: 0.4975 - val_loss: 0.6931 - val_accuracy: 0.5168\n",
      "Epoch 14/40\n",
      "400/400 [==============================] - 49s 122ms/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.4990\n",
      "Epoch 15/40\n",
      "400/400 [==============================] - 50s 125ms/step - loss: 0.6931 - accuracy: 0.5082 - val_loss: 0.6931 - val_accuracy: 0.4980\n",
      "Epoch 16/40\n",
      "400/400 [==============================] - 48s 119ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5010\n",
      "Epoch 17/40\n",
      "400/400 [==============================] - 48s 121ms/step - loss: 0.6931 - accuracy: 0.4950 - val_loss: 0.6931 - val_accuracy: 0.5040\n",
      "Epoch 18/40\n",
      "400/400 [==============================] - 66s 165ms/step - loss: 0.6931 - accuracy: 0.5098 - val_loss: 0.6931 - val_accuracy: 0.4842\n",
      "Epoch 19/40\n",
      "400/400 [==============================] - 63s 157ms/step - loss: 0.6931 - accuracy: 0.4918 - val_loss: 0.6931 - val_accuracy: 0.5208\n",
      "Epoch 20/40\n",
      "400/400 [==============================] - 49s 122ms/step - loss: 0.6931 - accuracy: 0.4996 - val_loss: 0.6931 - val_accuracy: 0.4970\n",
      "Epoch 21/40\n",
      "400/400 [==============================] - 56s 140ms/step - loss: 0.6931 - accuracy: 0.5005 - val_loss: 0.6931 - val_accuracy: 0.4980\n",
      "Epoch 22/40\n",
      "400/400 [==============================] - 100s 250ms/step - loss: 0.6931 - accuracy: 0.4944 - val_loss: 0.6931 - val_accuracy: 0.5079\n",
      "Epoch 23/40\n",
      "400/400 [==============================] - 114s 285ms/step - loss: 0.6931 - accuracy: 0.5074 - val_loss: 0.6931 - val_accuracy: 0.4941\n",
      "Epoch 24/40\n",
      "400/400 [==============================] - 113s 282ms/step - loss: 0.6931 - accuracy: 0.5020 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 25/40\n",
      "400/400 [==============================] - 114s 284ms/step - loss: 0.6931 - accuracy: 0.5086 - val_loss: 0.6931 - val_accuracy: 0.4960\n",
      "Epoch 26/40\n",
      "400/400 [==============================] - 118s 296ms/step - loss: 0.6931 - accuracy: 0.5014 - val_loss: 0.6931 - val_accuracy: 0.4970\n",
      "Epoch 27/40\n",
      "400/400 [==============================] - 70s 174ms/step - loss: 0.6931 - accuracy: 0.4994 - val_loss: 0.6931 - val_accuracy: 0.4990\n",
      "Epoch 28/40\n",
      "400/400 [==============================] - 67s 167ms/step - loss: 0.6931 - accuracy: 0.4938 - val_loss: 0.6931 - val_accuracy: 0.4881\n",
      "Epoch 29/40\n",
      "400/400 [==============================] - 49s 121ms/step - loss: 0.6931 - accuracy: 0.4925 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 30/40\n",
      "400/400 [==============================] - 50s 125ms/step - loss: 0.6931 - accuracy: 0.4976 - val_loss: 0.6931 - val_accuracy: 0.4832\n",
      "Epoch 31/40\n",
      "400/400 [==============================] - 47s 118ms/step - loss: 0.6931 - accuracy: 0.4939 - val_loss: 0.6931 - val_accuracy: 0.4901\n",
      "Epoch 32/40\n",
      "400/400 [==============================] - 47s 118ms/step - loss: 0.6931 - accuracy: 0.4981 - val_loss: 0.6931 - val_accuracy: 0.4970\n",
      "Epoch 33/40\n",
      "400/400 [==============================] - 47s 117ms/step - loss: 0.6931 - accuracy: 0.5105 - val_loss: 0.6931 - val_accuracy: 0.4970\n",
      "Epoch 34/40\n",
      "400/400 [==============================] - 47s 118ms/step - loss: 0.6931 - accuracy: 0.4958 - val_loss: 0.6931 - val_accuracy: 0.4941\n",
      "Epoch 35/40\n",
      "400/400 [==============================] - 47s 117ms/step - loss: 0.6931 - accuracy: 0.4984 - val_loss: 0.6931 - val_accuracy: 0.4980\n",
      "Epoch 36/40\n",
      "400/400 [==============================] - 50s 125ms/step - loss: 0.6931 - accuracy: 0.4956 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 37/40\n",
      "400/400 [==============================] - 46s 115ms/step - loss: 0.6931 - accuracy: 0.4955 - val_loss: 0.6931 - val_accuracy: 0.4881\n",
      "Epoch 38/40\n",
      "400/400 [==============================] - 48s 120ms/step - loss: 0.6931 - accuracy: 0.5011 - val_loss: 0.6931 - val_accuracy: 0.4960\n",
      "Epoch 39/40\n",
      "400/400 [==============================] - 46s 116ms/step - loss: 0.6931 - accuracy: 0.4952 - val_loss: 0.6931 - val_accuracy: 0.5030\n",
      "Epoch 40/40\n",
      "400/400 [==============================] - 49s 123ms/step - loss: 0.6931 - accuracy: 0.4918 - val_loss: 0.6931 - val_accuracy: 0.4990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16ed0f2dfd0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=n_training_sample // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=n_validation_sample // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#testing the model\n",
    "pred= image.load_img('data/dog_1.jpg', target_size=(150,150))\n",
    "pred=image.img_to_array(pred)\n",
    "pred= np.expand_dims(pred, axis=0)\n",
    "result= model.predict(pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The animal in the image is dog\n"
     ]
    }
   ],
   "source": [
    "if result[0][0]==1:\n",
    "    answer='dog'\n",
    "else:\n",
    "    answer='cat'\n",
    "print(\"The animal in the image is\",answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
